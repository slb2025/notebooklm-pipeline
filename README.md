# ðŸ§  AI Knowledge Pipeline for NotebookLM

Ce projet est un pipeline d'ingestion avancÃ© conÃ§u pour construire une base de connaissance IA de haute qualitÃ© ("Curated Dataset") optimisÃ©e pour **NotebookLM**.

Il transforme le web chaotique (Blogs Tech, News, Papiers de recherche) en une base de donnÃ©es Markdown structurÃ©e, propre et triÃ©e.

## âœ¨ FonctionnalitÃ©s ClÃ©s

*   **ðŸ•µï¸â€â™‚ï¸ Crawler "Stealth" & Robuste** : 
    *   **Playwright** : Utilise un vrai navigateur pour contourner les protections anti-bot (OpenAI, Google) et charger les contenus dynamiques (React, Infinite Scroll).
    *   **Anti-Bruit Natif** : Bloque automatiquement les URLs parasites (rÃ©seaux sociaux, pages d'index, archives annuelles) *avant* le tÃ©lÃ©chargement.
    *   **File Safety Net** : VÃ©rifie les titres pour Ã©viter de sauvegarder des pages "Latest News" ou "Search Results".
*   **ðŸ§  Classification Intelligente** : Analyse le contenu pour trier automatiquement les articles :
    *   `Generative AI` (LLMs, RAG...)
    *   `Deep Learning` (Theory, RLHF...)
    *   `Agentic AI` (Agents, Tools, MCP...)
    *   `Robotics` (Physical AI, Humanoid...)
    *   `Infrastructure` (Hardware, MLOps...)
*   **ðŸ“‚ Organisation Temporelle & Nommage** : 
    *   Structure : `Categorie/Sous-Categorie/AnnÃ©e/`
    *   Fichiers : `YYYY-MM-DD_sitename_titre_article.md` (Ex: `2024-03-22_google_research_why_agents_matter.md`).
*   **ðŸ›¡ï¸ QualitÃ© des DonnÃ©es** :
    *   Filtre strict : Ignore tout contenu antÃ©rieur Ã  **2023** (configurable).
    *   Nettoyage : Suppression des pubs, menus et scripts via `trafilatura`.
    *   Doublons : Scan intelligent de votre rÃ©pertoire local pour ne jamais tÃ©lÃ©charger deux fois le mÃªme article.

## ðŸ›  Architecture du Pipeline

1.  **Input** : Liste de sites dans `urls.txt` (Google, OpenAI, Anthropic, Meta, Nvidia, etc.).
2.  **Extract (Playwright)** : Navigation "humaine", scroll infini, blocage des ressources lourdes (Images/Fonts) pour la performance.
3.  **Filter & Transform** : 
    *   Filtrage des URLs "bruit" (Twitter, Facebook, Index pages).
    *   Classification par mots-clÃ©s.
    *   GÃ©nÃ©ration de nom de fichier canonique (Date + Site + Titre).
4.  **Load** : Sauvegarde dans Google Drive avec une arborescence triÃ©e par annÃ©e.

## ðŸš€ Installation & Usage

### 1. PrÃ©requis
*   Python 3.8+
*   Un environnement virtuel recommandÃ©

### 2. Installation
```bash
# Installation des dÃ©pendances
pip install playwright trafilatura dateparser

# Installation des navigateurs pour Playwright
playwright install chromium
```

### 3. Configuration
*   Ã‰ditez `urls.txt` pour ajouter vos sources prÃ©fÃ©rÃ©es.
*   (Optionnel) Modifiez `OUTPUT_DIR` dans `ingest_auto_crawl.py` pour pointer vers votre dossier Drive local.

### 4. Lancement (Pipeline Automatique)
```bash
python ingest_auto_crawl.py
```
Le crawler va scanner les URLs, filtrer les pollueurs, vÃ©rifier les dates, et sauvegarder le contenu pertinent.

### 5. Outils ComplÃ©mentaires

#### ðŸŽ¯ Import Manuel (Mode Sniper)
Pour ajouter un article spÃ©cifique sans relancer tout le crawl (utilise la mÃªme logique de classification) :
```bash
python ingest_manual.py
# Puis collez l'URL quand demandÃ©
```

#### ðŸ§¹ Nettoyage (Anti-Bruit Interactif)
Si jamais des fichiers indÃ©sirables sont passÃ©s :
```bash
python clean_noise.py
```
*   Mode interactif : Vous liste les fichiers suspects.
*   Suppression par lot (si > 30 fichiers) ou un par un.

## ðŸ“‚ Structure des Dossiers (Exemple)

```text
NotebookLM_Sources/
â”œâ”€â”€ Generative AI/
â”‚   â””â”€â”€ LLMs/
â”‚       â”œâ”€â”€ 2024/
â”‚       â”‚   â””â”€â”€ 2024-03-15_openai_gpt4_technical_report.md
â”‚       â””â”€â”€ 2025/
â”‚           â””â”€â”€ 2025-01-10_google_research_gemini_ultra_update.md
â”œâ”€â”€ Agentic AI/
â”‚   â””â”€â”€ Agents/
â”‚       â””â”€â”€ 2025/
â”‚           â””â”€â”€ 2025-02-12_anthropic_building_effective_agents.md
â”œâ”€â”€ Deep Learning/
â”‚   â””â”€â”€ Theory/
â”‚       â””â”€â”€ Undated/
â”‚           â””â”€â”€ Undated_web_introduction_to_transformers.md
```